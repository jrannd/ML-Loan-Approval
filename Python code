#%% 
# import packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#%% 
df=pd.read_csv('loan_data.csv')
#%% 
df
#%% 
# explore data
df.info() 
# if dat is complete then there will be 381 entries in each column
# drop null values in dataset
#%% 
df[df.isnull().any(axis=1)]
# 73 total null values
#%% 
df.dropna(inplace=True)
df
#%% 
# Visualize interesting data points
gender_counts=df["Gender"].value_counts()
fig = plt.figure(figsize = (18,12))

ax1=plt.subplot(2,3,1)
plt.bar(gender_counts.index, gender_counts)

ax2=plt.subplot(2,3,2)
plt.bar(gen_loan.index, gen_loan)

ax3=plt.subplot(2,3,3)
plt.hist(df['ApplicantIncome'])

ax4=plt.subplot(2,3,4)
plt.hist(df['LoanAmount'])

plt.show()
#%% 
df["Gender"].value_counts()
#%% 
gen_loan=df.groupby("Gender")["LoanAmount"].sum()
gen_loan
#%% 
# prepare data for ML
from sklearn.preprocessing import LabelEncoder
#%% 
le=LabelEncoder()
#%% 
# fit label encoder to text columns
le.fit(df['Gender']) # female = 0 and male 1
le.classes_
#%% 
# Input the labels into df
df['Gender']=le.transform(df["Gender"])
#%% 
le.fit(df['Married'])
le.classes_
#%% 
df['Married']=le.transform(df["Married"])
#%% 
le.fit(df['Education'])
le.classes_
#%% 
df['Education']=le.transform(df["Education"])
#%% 
le.fit(df['Self_Employed'])
le.classes_
#%% 
df['Self_Employed']=le.transform(df["Self_Employed"])
#%% 
le.fit(df['Property_Area'])
le.classes_
#%% 
df['Property_Area']=le.transform(df["Property_Area"])
#%% 
le.fit(df['Loan_Status'])
le.classes_
#%% 
df['Loan_Status_y']=le.transform(df["Loan_Status"])
#%% 
#Replace 3+ in Dependents column for df
df['Dependents']=df['Dependents'].replace({'3+':'3'})
df["Dependents"].astype(int)
df.head()
#%% 
# Create Train and Test dataframes
from sklearn.model_selection import train_test_split
train_df,test_df=train_test_split(df,test_size=0.2)
train_df
#%% 
test_df
#%% 
# Now we put all the features into a training and test dataset and the same with the target
X_train = train_df.iloc[:,1:12]
X_test = test_df.iloc[:,1:12]
#%% 
y_train = train_df.iloc[:,-1:]
y_test = test_df.iloc[:,-1:]
#%% 
# Now we need to scale out features using StandardScaler
from sklearn.preprocessing import StandardScaler
StdScaler = StandardScaler()
StdScaler.fit(X_train)
#%% 
X_train_scaled = StdScaler.transform(X_train)
X_test_scaled = StdScaler.transform(X_test)
#%% 
# Use classifier to train model
from sklearn.linear_model import SGDClassifier
SGD_clf=SGDClassifier(n_jobs=-1)
SGD_clf.fit(X_train_scaled,y_train)
#%% 
# predict if loan approvals
predicted_la=SGD_clf.predict(X_test_scaled)
#%% 
# It worked, we have predicted values
predicted_la
#%% 
# Scoring and confusion matrix
from sklearn.metrics import recall_score, precision_score,f1_score,accuracy_score
accuracy=accuracy_score(y_true=y_test,y_pred=predicted_la)
accuracy
# 87% of the predictions are correct
#%% 
rs=recall_score(y_true=y_test,y_pred=predicted_la)
rs
# True positive rate: ratio of positive instances that are correctly detected by the classifier. Of the true instances (someone getting a loan) the model was correct 97% of the time classified 40 out of 41 instances correctly
#%% 
f1=f1_score(y_true=y_test,y_pred=predicted_la)
f1
# F1 is harmonious mean between recall and precision 
#%% 
ps=precision_score(y_true=y_test,y_pred=predicted_la)
ps
# Precision is accuracy of positive predicitions out of 47 positive instances, we guessed 40 correctly. 85% of our positive predictions were actually correct.
#%% 
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
con_matrix = confusion_matrix(y_test,predicted_la)
con_matrixFig = ConfusionMatrixDisplay(confusion_matrix=con_matrix, display_labels=["Is not A (N)", "Is A (P)"])
con_matrix
#%% 
fig = plt.figure()
ax1=plt.subplot(1,1,1)
con_matrixFig.plot(ax=ax1)
plt.show()
#%% 
# TN is 14
# TP is 40 
# FN is 1
# FP is 7
#%% 
#ROC CURVE
from sklearn.metrics import roc_curve
y_scores=SGD_clf.decision_function(X_test_scaled)
fpr,tpr,thresholds=roc_curve(y_true=y_test,y_score=y_scores)
#%% 
fig = plt.figure(figsize=(10,6))

ax1=plt.subplot(1,1,1)
plt.plot(fpr,tpr, c="blue")
plt.plot([0,1],[0,1],c="black",ls="dashed", lw=3)
plt.grid(True)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

plt.show()
# this plots true positve rate and false positive rate
#%% 
# AOC
from sklearn.metrics import roc_auc_score
roc_auc_score(y_true=y_test,y_score=y_scores)
# roc_AUC score shows how this classifier is no making random guesses (random guesses are 0.5) but making correct decisions. 
#%% 

#%% 
